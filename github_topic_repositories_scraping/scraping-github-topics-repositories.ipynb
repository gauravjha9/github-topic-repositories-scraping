{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping `GitHub Topics` Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/github_logo.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the `PROJECT`\n",
    "- The main `Objective` of the Project is to Scrape `Github Topics` Page and get the information of Each Topic (Title, URL, Description).\n",
    "\n",
    "- Then For each Topic get the Top 30 Repositories information (Name, Username, Stars, URL).\n",
    "\n",
    "- And then Save it to CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "1. Web Scraping: \n",
    "\n",
    "    - Web scraping is a technique to fetch data from websites. It is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications.\n",
    "<br><br/>\n",
    "2. GitHub\n",
    "    - GitHub is a web-based interface that uses Git, the open source version control software that lets multiple people make separate changes to web pages at the same time.\n",
    "    - In this Project, I'll be need GitHub link to Scrape the Topics Repositories.\n",
    "<br><br/>  \n",
    "3. Tools/Libraries (Used in this Project)    \n",
    "    - Python: For `Scripting`.\n",
    "    - requests: To `Download` the website (HTML/CSS) using url.\n",
    "    - Beatiful Soup: To `Parse` the page.\n",
    "    - pandas: To Convert all the data into `DataFrame` and then save it as `.CSV` file.\n",
    "    - os: To Make Directory and save all the CSV file into the Directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Outline:\n",
    "\n",
    "- Scrape this website: https://github.com/topics\n",
    "- Get the list of Topics. For each Topic get Topic Title, Topic page URL and Topic description.\n",
    "- For each Topic, Get the top 30 Repositories in the Topic from the Topic page.\n",
    "- For each Repository, grab the Repository name, username, Stars and Repository URL.\n",
    "- For each Topic Create a CSV file in the following format:\n",
    "```\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,81900,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,20000,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import all the required Libraries.\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `requests` to `Download` the page\n",
    "- Use `bs4` to to `Parse` the page\n",
    "- Use `Pandas` to Convert Pandas Dataframe and save as Data to `.CSV` file\n",
    "- Use `os` to Make Directory and save all the `.CSV` file in the Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the list of Topics from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_page():\n",
    "    \"\"\"\n",
    "    This Function simply Takes Topics URL and Download the page content as HTML text, \n",
    "    Parse using Beautiful Soup and return all the text content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download the page\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topics_url}')\n",
    "    \n",
    "    # Parse using Beautiful Soup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calling the get_topics_page function and store return value in a variable.\n",
    "\"\"\"\n",
    "\n",
    "doc = get_topics_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some Helper functions to Parse information from the Page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Topic Titles\n",
    "\n",
    "To get topic titles, we can pick `p` tags with the class `f3 lh-condensed mb-0 mt-1 Link--primary`\n",
    "\n",
    "<img src='images/title_tag.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    \"\"\"\n",
    "    This function takes all the content (doc) and,\n",
    "    Parse the topic title using Beautiful Soup,\n",
    "    and store all the topic title into a List and return that list.\n",
    "    \"\"\"\n",
    "    \n",
    "    title_selector_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': title_selector_class})    \n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_topic_titles` used to get the List of Topic Titles.\n",
    "\n",
    "Explanation:\n",
    "- Open the URL `https://github.com/topics` in a Browser then Inspect the topic title and observe that topic title was inside the `p` tag with the class `f3 lh-condensed mb-0 mt-1 Link--primary`.\n",
    "<br><br/>\n",
    "- Then using Beautiful Soup `find_all` method get all `p` tags with the class name `f3 lh-condensed mb-0 mt-1 Link--primary`. \n",
    "<br><br/>\n",
    "- Using Loop store all the `p` tags text into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calling the get_topic_titles function\n",
    "\"\"\"\n",
    "titles = get_topic_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android', 'Angular', 'Ansible', 'API', 'Arduino', 'ASP.NET', 'Atom', 'Awesome Lists', 'Amazon Web Services', 'Azure', 'Babel', 'Bash', 'Bitcoin', 'Bootstrap', 'Bot', 'C', 'Chrome', 'Chrome extension', 'Command line interface', 'Clojure', 'Code quality', 'Code review', 'Compiler', 'Continuous integration', 'COVID-19', 'C++']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read all the Topic title.\n",
    "\"\"\"\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Topic Description\n",
    "\n",
    "To get topic description, we can pick `p` tags with the class `f5 color-fg-muted mb-0 mt-1`\n",
    "\n",
    "<img src='images/description_tag.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descriptions(doc):\n",
    "    \"\"\"\n",
    "    Similarly, This function takes all the content (doc) and,\n",
    "    Parse the topic Description using Beautiful Soup,\n",
    "    and store all the topics Description into a List and return that list.\n",
    "    \"\"\"\n",
    "    \n",
    "    description_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_description_tags = doc.find_all('p', {'class': description_selector})\n",
    "    topic_descriptions = []\n",
    "    for tag in topic_description_tags:\n",
    "        topic_descriptions.append(tag.text.strip())\n",
    "    return topic_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_topic_descriptions` used to get the List of Topics Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calling the get_topic_description function.\n",
    "\"\"\"\n",
    "descriptions = get_topic_descriptions(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D modeling is the process of virtually developing the surface and structure of a 3D object.',\n",
       " 'Ajax is a technique for creating interactive web applications.',\n",
       " 'Algorithms are self-contained sequences that carry out a variety of tasks.',\n",
       " 'Amp is a non-blocking concurrency library for PHP.',\n",
       " 'Android is an operating system built by Google designed for mobile devices.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read first 5 Topic Descriptions\n",
    "\"\"\"\n",
    "descriptions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Topic URL\n",
    "\n",
    "To get topic URL, we can pick `a` tags with the class `no-underline flex-1 d-flex flex-column`\n",
    "<img src='images/link_tag.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes all the content (doc) and,\n",
    "Parse the topic URL using Beautiful Soup,\n",
    "and store all the topic URL into a List and return that list.\n",
    "\"\"\"\n",
    "\n",
    "def get_topic_url(doc):\n",
    "    url_selector = 'no-underline flex-1 d-flex flex-column'\n",
    "    topic_link_tags = doc.find_all('a', {'class': url_selector})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(f\"{base_url}{tag['href']}\")\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calling the get_topic_url function.\n",
    "\"\"\"\n",
    "urls = get_topic_url(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/topics/3d',\n",
       " 'https://github.com/topics/ajax',\n",
       " 'https://github.com/topics/algorithm',\n",
       " 'https://github.com/topics/amphp',\n",
       " 'https://github.com/topics/android']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read first 5 Topic URLs\n",
    "\"\"\"\n",
    "urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now put this all together into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    \"\"\"\n",
    "    In this function call each Helper function \n",
    "    and store the returned value in a Dictionary.\n",
    "    Then convert the Dictionary into DataFrame and \n",
    "    simply return DataFrame.\n",
    "    \"\"\"\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    topics_dict = {\n",
    "        'Title': get_topic_titles(doc),\n",
    "        'Description': get_topic_descriptions(doc),\n",
    "        'URL': get_topic_url(doc)\n",
    "    }     \n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calling scrape_topics function and\n",
    "store returned value in a variable.\n",
    "\"\"\"\n",
    "topics_df = scrape_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D modeling is the process of virtually develo...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>Amp is a non-blocking concurrency library for ...</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                        Description  \\\n",
       "0         3D  3D modeling is the process of virtually develo...   \n",
       "1       Ajax  Ajax is a technique for creating interactive w...   \n",
       "2  Algorithm  Algorithms are self-contained sequences that c...   \n",
       "3        Amp  Amp is a non-blocking concurrency library for ...   \n",
       "4    Android  Android is an operating system built by Google...   \n",
       "\n",
       "                                   URL  \n",
       "0         https://github.com/topics/3d  \n",
       "1       https://github.com/topics/ajax  \n",
       "2  https://github.com/topics/algorithm  \n",
       "3      https://github.com/topics/amphp  \n",
       "4    https://github.com/topics/android  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read first 5 rows from the DataFrame.\n",
    "\"\"\"\n",
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the top repositories from a topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    \"\"\"\n",
    "    This function Take topic url, Download all the content\n",
    "    from the url using requests library and Parse it using \n",
    "    Beatiful Soup then return all the content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # download the page\n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    \n",
    "    # Parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the required information from the topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    \"\"\"\n",
    "    This is helper function, It takes counted\n",
    "    star as string convert it into integer and\n",
    "    return the star count value.\n",
    "    \"\"\"\n",
    "    \n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag, star_tag):\n",
    "    \"\"\"\n",
    "    This function takes two arguments (h3_tag, star tag)\n",
    "    using h3_tags Get the information (username, repo_name, repo_url)\n",
    "    using star_tag get the counted star and return all the values.\n",
    "    \"\"\"\n",
    "\n",
    "    a_tags = h3_tag.find_all('a')\n",
    "\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    base_url = 'https://github.com'\n",
    "    repo_url =  f'{base_url}{a_tags[1][\"href\"]}'\n",
    "    stars = parse_star_count(star_tag.text)\n",
    "    \n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- In the above function h3_tag argument contains Username, Repository Name and Repo URL\n",
    "- So using Beautiful Soup get all the Element and add Base URL in the repo url.\n",
    "- The 2nd argument star_tag contains the counted star element, get text from it\n",
    "- Then using parse_star_count function just convert string value to Integer.\n",
    "- Finally return username, repo_name, stars, repo_url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \"\"\"\n",
    "    It takes topic doc (all content of topic page),\n",
    "    Find username, repo_name, stars, repo_url and,\n",
    "    Create dictionary then using loop store all the values \n",
    "    in list and assing all the list as Dictionary value.\n",
    "    Convert the dictionary to Dataframe and return.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the h3 tags containing repo title, repo URL and username\n",
    "    h3_selector = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h3_selector})\n",
    "    \n",
    "    # Get the star tag\n",
    "    span_selector = 'Counter js-social-count'\n",
    "    star_tags = topic_doc.find_all('span', {'class': span_selector})\n",
    "    \n",
    "    \n",
    "    topic_repos_dict = {\n",
    "        'username': [],\n",
    "        'repo_name': [],\n",
    "        'stars': [],\n",
    "        'repo_url': []\n",
    "    }\n",
    "    \n",
    "    # Get repo info\n",
    "    for repo in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[repo], star_tags[repo])\n",
    "\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "    \"\"\"\n",
    "    It takes two arguments (topic_url, path),\n",
    "    check if path already exist then show a message otherwise,\n",
    "    call get_topic_repos() function and paas another function \n",
    "    as argument then save a returned value as .csv file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f'The file {path} already exist. Skipping...')\n",
    "        return \n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Putting it all together\n",
    "\n",
    "- Have a function to get the List of Topics `get_topic_repos(topic_doc)`\n",
    "- Have a function to Create a CSV file for scrape repository from a topics page `scrape_topic(topic_url, path)`\n",
    "- Now Create a function to put them together `scrape_topics_repos()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    \"\"\"\n",
    "    In this function Call scrape_topics() and store returned \n",
    "    value(Topics DataFrame) in variable. Then Create a Folder\n",
    "    name Data. Using loop iterate each row and call scrape_topic\n",
    "    function for next Process.\n",
    "    \"\"\"\n",
    "    print(\"Scraping list of topics\")\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True) \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print(f'Scraping Top repositories for {row[\"Title\"]} to {row[\"URL\"]}')\n",
    "        url = row['URL']\n",
    "        path = f\"data/{row['Title']}.csv\"\n",
    "        scrape_topic(url, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repos for all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping Top repositories for 3D to https://github.com/topics/3d\n",
      "Scraping Top repositories for Ajax to https://github.com/topics/ajax\n",
      "Scraping Top repositories for Algorithm to https://github.com/topics/algorithm\n",
      "Scraping Top repositories for Amp to https://github.com/topics/amphp\n",
      "Scraping Top repositories for Android to https://github.com/topics/android\n",
      "Scraping Top repositories for Angular to https://github.com/topics/angular\n",
      "Scraping Top repositories for Ansible to https://github.com/topics/ansible\n",
      "Scraping Top repositories for API to https://github.com/topics/api\n",
      "Scraping Top repositories for Arduino to https://github.com/topics/arduino\n",
      "Scraping Top repositories for ASP.NET to https://github.com/topics/aspnet\n",
      "Scraping Top repositories for Atom to https://github.com/topics/atom\n",
      "Scraping Top repositories for Awesome Lists to https://github.com/topics/awesome\n",
      "Scraping Top repositories for Amazon Web Services to https://github.com/topics/aws\n",
      "Scraping Top repositories for Azure to https://github.com/topics/azure\n",
      "Scraping Top repositories for Babel to https://github.com/topics/babel\n",
      "Scraping Top repositories for Bash to https://github.com/topics/bash\n",
      "Scraping Top repositories for Bitcoin to https://github.com/topics/bitcoin\n",
      "Scraping Top repositories for Bootstrap to https://github.com/topics/bootstrap\n",
      "Scraping Top repositories for Bot to https://github.com/topics/bot\n",
      "Scraping Top repositories for C to https://github.com/topics/c\n",
      "Scraping Top repositories for Chrome to https://github.com/topics/chrome\n",
      "Scraping Top repositories for Chrome extension to https://github.com/topics/chrome-extension\n",
      "Scraping Top repositories for Command line interface to https://github.com/topics/cli\n",
      "Scraping Top repositories for Clojure to https://github.com/topics/clojure\n",
      "Scraping Top repositories for Code quality to https://github.com/topics/code-quality\n",
      "Scraping Top repositories for Code review to https://github.com/topics/code-review\n",
      "Scraping Top repositories for Compiler to https://github.com/topics/compiler\n",
      "Scraping Top repositories for Continuous integration to https://github.com/topics/continuous-integration\n",
      "Scraping Top repositories for COVID-19 to https://github.com/topics/covid-19\n",
      "Scraping Top repositories for C++ to https://github.com/topics/cpp\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the CSVs were created properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
